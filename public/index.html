<!DOCTYPE html>
<html>
	<head lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>dhmnr.sh | Home </title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="" />
	<meta property="og:image" content=""/>
	<link rel="alternate" type="application/rss+xml" href="http://localhost:1313/index.xml" title="dhmnr.sh" />
	<meta property="og:url" content="http://localhost:1313/">
  <meta property="og:site_name" content="dhmnr.sh">
  <meta property="og:title" content="dhmnr.sh">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="website">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="dhmnr.sh">

        <link href="http://localhost:1313/css/fonts.11a1877508139eac0b5b4852ceb110c35641b3533321e66e39149e901ed5756b.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="http://localhost:1313/css/main.2d8b5d025466bd6ddaba53abadd82452f7b979866a58623830289b480f93bb3b.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="http://localhost:1313/css/dark.3cc53e265be0aadc134ff08f276085599313959408ee8c32d9cf35ce71670fff.css"  disabled /><link rel="stylesheet" href="http://localhost:1313/katex/katex.min.css ">
		<script defer src="http://localhost:1313/katex/katex.min.js"></script>
		<script defer src="http://localhost:1313/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
		</script>
		
		
		<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/jetbrains-mono.027bd4d70293c64700e7ad92bd136797198199ca99c9ea54541f4b41805e9006.css">
		
		
		<link rel="stylesheet" type="text/css" href="http://localhost:1313/css/speedrun.ff7cd40ab2a8b7b70c253c14e084f61b5d4ce5c3190ea3b5c440e26f0fa3a52d.css">
		
</head>

	<body>
		<div class="content">
			<header>
	<div class="main">
		<a href="http://localhost:1313/">dhmnr.sh</a>
	</div>
	<nav>
		
		<a href="/">home</a>
		
		<a href="/posts">posts</a>
		
		<a href="/about">about</a>
		
		<a href="/tags">tags</a>
		
		<button id="dark-mode-toggle" class="nav-toggle" onclick="toggleTheme()" aria-label="Toggle dark mode" type="button"><svg class="feather" viewBox="0 0 24 24" fill="none" stroke="#232333" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
      <circle cx="12" cy="12" r="5"></circle>
      <line x1="12" y1="1" x2="12" y2="3"></line>
      <line x1="12" y1="21" x2="12" y2="23"></line>
      <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
      <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
      <line x1="1" y1="12" x2="3" y2="12"></line>
      <line x1="21" y1="12" x2="23" y2="12"></line>
      <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
      <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
    </svg></button>
		<script src="http://localhost:1313/js/themetoggle.js"></script>
		
	</nav>
</header>

			
			<main class="list">
				<div class="site-description"><p>everything.</p></div>
				
				
				
				<section class="list-item">
					<h1 class="title"><a href="/posts/intro-to-ptx-optimization/">Intro to PTX Optimization : Part 1</a></h1>
					<time>Nov 12, 2025 <span class="draft-label">DRAFT</span> </time>
					<br><div class="description">
	
	<p>If you&rsquo;re trying to push your GPU harder than CUDA C++ allows, eventually you end up at PTX. It&rsquo;s NVIDIA&rsquo;s intermediate representation, sitting between your kernel code and the hardware.
This post covers PTX from the ground up: what it is, how to write it by hand, and why it matters. Make sure you&rsquo;re comfortable with CUDA kernels and the memory model before jumping in.</p>
<hr>
<h2 id="what-is-ptx">What is PTX?</h2>
<p>PTX is a low-level, virtual instruction set designed by NVIDIA for their GPUs. It&rsquo;s basically the GPU&rsquo;s &ldquo;assembly language,&rdquo; but it&rsquo;s not the final machine code. Instead, PTX gets compiled just-in-time (JIT) by the NVIDIA driver into SASS, the actual binary that runs on your specific GPU architecture.</p>&hellip;
	
</div>
					<a class="readmore" href="/posts/intro-to-ptx-optimization/">Read more ⟶</a>
				</section>
				
				<section class="list-item">
					<h1 class="title"><a href="/posts/tenets-of-agi/">Tenets of Human-Like AGI</a></h1>
					<time>Oct 30, 2025</time>
					<br><div class="description">
	
	<p>This post attempts to define what human-like AGI might look like. These tenets are rooted in emulating biological intelligence and overcoming its drawbacks.</p>
<h2 id="working-definition-of-agi">Working definition of AGI</h2>
<p>AGI is a system that achieves human-level general intelligence, performing at or near human capability across a broad range of domains. The defining feature is generality rather than superhuman performance in any particular area. This is different from Narrow AI, which is superhuman at specific tasks but subhuman at generalization, and from ASI, which is far beyond human capability across all domains.</p>&hellip;
	
</div>
					<a class="readmore" href="/posts/tenets-of-agi/">Read more ⟶</a>
				</section>
				
				<section class="list-item">
					<h1 class="title"><a href="/posts/speedrunning-ncu/">Speedrunning GPU Profiling with Nsight Compute CLI</a></h1>
					<time>Oct 27, 2025 <span class="draft-label">DRAFT</span> </time>
					<br><div class="description">
	
	<div class="speedrun-definition">
    <h2 class="speedrun-title">speed·running <span class="pronunciation">/ˈspēdˌrəniNG/</span> <em>v.</em></h2>
    <p class="definition-text">
        <strong>1.</strong> To complete (a video game, or part of a game) as fast as possible. Speedrunning often
        involves following planned routes, which may incorporate sequence breaking and allow
        sections to be skipped.
    </p>
</div>
<p>You&rsquo;ve written your first CUDA kernel. You even implemented a basic reduction. It runs. But is it fast? Is it optimized? Where are the bottlenecks? How do you even answer these question?</p>
<br>
<p>The Answer is Profiling.</p>&hellip;
	
</div>
					<a class="readmore" href="/posts/speedrunning-ncu/">Read more ⟶</a>
				</section>
				
				<section class="list-item">
					<h1 class="title"><a href="/posts/a-higher-level-intelligence/">A Higher Level Intelligence</a></h1>
					<time>Oct 21, 2025 <span class="draft-label">DRAFT</span> </time>
					<br><div class="description">
	
	<h2 id="i-the-core-hypothesis">I. The Core Hypothesis</h2>
<p>Current paradigm:</p>
<p>Reasoning = manipulating discrete symbolic tokens (words)
Intelligence = learning patterns in token sequences
Architecture: Attention over discrete tokens</p>
<p>Our hypothesis:</p>
<p>Reasoning occurs in a continuous pre-linguistic substrate
Language is a lossy discretization of this substrate for communication
Current AI learns from the discretization, never accessing the underlying continuous process
This is why they can be fluent but not truly reason</p>
<p>Mathematical statement:
There exists a continuous cognitive process $\Psi(x, \alpha, t)$ where:</p>&hellip;
	
</div>
					<a class="readmore" href="/posts/a-higher-level-intelligence/">Read more ⟶</a>
				</section>
				
				<section class="list-item">
					<h1 class="title"><a href="/posts/serving-llms-at-scale-flashattention/">Serving LLMs at Scale: FlashAttention</a></h1>
					<time>Oct 17, 2025 <span class="draft-label">DRAFT</span> </time>
					<br><div class="description">
	
	<p>Attention is the bottleneck in transformer inference, but not for the reasons you might think.
A100 sits at 30% utilization during attention not because the math is hard, but because
it&rsquo;s spending most of its time waiting for data to move between memory and compute cores.
FlashAttention fixes this by keeping computation in fast on-chip SRAM and never materializing
the O(N²) attention matrix. Here&rsquo;s how it works.</p>&hellip;
	
</div>
					<a class="readmore" href="/posts/serving-llms-at-scale-flashattention/">Read more ⟶</a>
				</section>
				
				<section class="list-item">
					<h1 class="title"><a href="/posts/an-empty-post/">An Empty Post</a></h1>
					<time>Oct 15, 2025</time>
					<br><div class="description">
	
	&hellip;
	
</div>
					<a class="readmore" href="/posts/an-empty-post/">Read more ⟶</a>
				</section>
				
				


			</main>
			<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2026  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>


		</div>
		
	</body>
</html>
