<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on dhmnr.sh</title>
    <link>https://dhmnr.sh/posts/</link>
    <description>Recent content in Posts on dhmnr.sh</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 17 Oct 2025 23:46:20 -0700</lastBuildDate>
    <atom:link href="https://dhmnr.sh/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>All About Attention: From Textbook to Production</title>
      <link>https://dhmnr.sh/posts/complete-math-behind-llms/</link>
      <pubDate>Fri, 17 Oct 2025 23:46:20 -0700</pubDate>
      <guid>https://dhmnr.sh/posts/complete-math-behind-llms/</guid>
      <description>&lt;p&gt;In 2017, a team at Google published a paper with a bold title: &amp;ldquo;Attention Is All You Need.&amp;rdquo; They weren&amp;rsquo;t exaggerating. The attention mechanism introduced in that paper fundamentally transformed how we build AI systems. Today, every major language model, from GPT to Claude to Gemini, has attention at its core.&#xA;But here&amp;rsquo;s the thing: the elegant equations you see in textbooks and the attention mechanisms running in production are surprisingly different. The gap between theory and practice is vast, and bridging it requires understanding not just the mathematics, but the hardware constraints, memory hierarchies, and engineering innovations that make modern LLMs possible.&#xA;This post is that bridge.&lt;/p&gt;</description>
    </item>
    <item>
      <title>An Empty Post</title>
      <link>https://dhmnr.sh/posts/an-empty-post/</link>
      <pubDate>Wed, 15 Oct 2025 21:44:23 -0700</pubDate>
      <guid>https://dhmnr.sh/posts/an-empty-post/</guid>
      <description></description>
    </item>
  </channel>
</rss>
