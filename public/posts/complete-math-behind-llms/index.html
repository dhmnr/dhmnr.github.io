<!DOCTYPE html>
<html><head lang="en">
	<meta charset="utf-8" />
	<meta http-equiv="X-UA-Compatible" content="IE=edge"><title>All About Attention: From Textbook to Production - dhmnr.sh</title><meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="description" content="In 2017, a team at Google published a paper with a bold title: &ldquo;Attention Is All You Need.&rdquo; They weren&rsquo;t exaggerating. The attention mechanism introduced in that paper fundamentally transformed how we build AI systems. Today, every major language model, from GPT to Claude to Gemini, has attention at its core.
But here&rsquo;s the thing: the elegant equations you see in textbooks and the attention mechanisms running in production are surprisingly different. The gap between theory and practice is vast, and bridging it requires understanding not just the mathematics, but the hardware constraints, memory hierarchies, and engineering innovations that make modern LLMs possible.
This post is that bridge." />
	<meta property="og:image" content=""/>
	<meta property="og:url" content="https://dhmnr.sh/posts/complete-math-behind-llms/">
  <meta property="og:site_name" content="dhmnr.sh">
  <meta property="og:title" content="All About Attention: From Textbook to Production">
  <meta property="og:description" content="In 2017, a team at Google published a paper with a bold title: “Attention Is All You Need.” They weren’t exaggerating. The attention mechanism introduced in that paper fundamentally transformed how we build AI systems. Today, every major language model, from GPT to Claude to Gemini, has attention at its core. But here’s the thing: the elegant equations you see in textbooks and the attention mechanisms running in production are surprisingly different. The gap between theory and practice is vast, and bridging it requires understanding not just the mathematics, but the hardware constraints, memory hierarchies, and engineering innovations that make modern LLMs possible. This post is that bridge.">
  <meta property="og:locale" content="en_us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2025-10-17T23:46:20-07:00">
    <meta property="article:modified_time" content="2025-10-17T23:46:20-07:00">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="All About Attention: From Textbook to Production">
  <meta name="twitter:description" content="In 2017, a team at Google published a paper with a bold title: “Attention Is All You Need.” They weren’t exaggerating. The attention mechanism introduced in that paper fundamentally transformed how we build AI systems. Today, every major language model, from GPT to Claude to Gemini, has attention at its core. But here’s the thing: the elegant equations you see in textbooks and the attention mechanisms running in production are surprisingly different. The gap between theory and practice is vast, and bridging it requires understanding not just the mathematics, but the hardware constraints, memory hierarchies, and engineering innovations that make modern LLMs possible. This post is that bridge.">

        <link href="https://dhmnr.sh/css/fonts.11a1877508139eac0b5b4852ceb110c35641b3533321e66e39149e901ed5756b.css" rel="stylesheet">
	

	
	<link rel="stylesheet" type="text/css" media="screen" href="https://dhmnr.sh/css/main.2d8b5d025466bd6ddaba53abadd82452f7b979866a58623830289b480f93bb3b.css" />
		<link id="darkModeStyle" rel="stylesheet" type="text/css" href="https://dhmnr.sh/css/dark.3cc53e265be0aadc134ff08f276085599313959408ee8c32d9cf35ce71670fff.css"   /><link rel="stylesheet" href="https://dhmnr.sh/katex/katex.min.css ">
		<script defer src="https://dhmnr.sh/katex/katex.min.js"></script>
		<script defer src="https://dhmnr.sh/katex/auto-render.min.js" onload="renderMathInElement(document.body);"></script>
		
		<script>
			document.addEventListener("DOMContentLoaded", function() {
					renderMathInElement(document.body, {
							delimiters: [
									{left: "$$", right: "$$", display: true},
									{left: "$", right: "$", display: false}
							]
					});
			});
		</script>
</head>
<body>
        <div class="content"><header>
	<div class="main">
		<a href="https://dhmnr.sh/">dhmnr.sh</a>
	</div>
	<nav>
		
		<a href="/">home</a>
		
		<a href="/posts">posts</a>
		
		<a href="/about">about</a>
		
		<a href="/tags">tags</a>
		
		
	</nav>
</header>

<main>
  <article>
    <div class="post-container">
      
      <div class="post-content">
        <div class="title">
          <h1 class="title">All About Attention: From Textbook to Production</h1>
          <div class="meta">Posted on Oct 17, 2025 <span class="draft-label">DRAFT</span> </div>
        </div>
        
        <section class="body">
          <p>In 2017, a team at Google published a paper with a bold title: &ldquo;Attention Is All You Need.&rdquo; They weren&rsquo;t exaggerating. The attention mechanism introduced in that paper fundamentally transformed how we build AI systems. Today, every major language model, from GPT to Claude to Gemini, has attention at its core.
But here&rsquo;s the thing: the elegant equations you see in textbooks and the attention mechanisms running in production are surprisingly different. The gap between theory and practice is vast, and bridging it requires understanding not just the mathematics, but the hardware constraints, memory hierarchies, and engineering innovations that make modern LLMs possible.
This post is that bridge.</p>
<h2 id="part-1-textbook---understanding-attention">Part 1: Textbook - Understanding Attention</h2>
<h2 id="part-2-reality---why-textbook-attention-breaks">Part 2: Reality - Why Textbook Attention Breaks</h2>
<h2 id="part-3-evolution---what-we-tried">Part 3: Evolution - What We Tried</h2>
<h2 id="part-4-production---what-actually-works">Part 4: Production - What Actually Works</h2>
<h2 id="part-5-implementing-it-yourself">Part 5: Implementing It Yourself</h2>
<h2 id="part-6-practical-considerations">Part 6: Practical Considerations</h2>
<h2 id="conclusion">Conclusion</h2>
<h2 id="references">References</h2>

        </section>
        <div class="post-tags">
          
          
          
        </div>
      </div>

      
      
    </div>

    </article>
</main>
<footer>
  <div style="display:flex"></div>
  <div class="footer-info">
    2025  <a
      href="https://github.com/athul/archie">Archie Theme</a> | Built with <a href="https://gohugo.io">Hugo</a>
  </div>
</footer>



</div>
    </body>
</html>
